{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import opt_einsum as oe\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "from scipy import integrate\n",
    "import pickle as pk\n",
    "import copy\n",
    "import math\n",
    "import scipy\n",
    "import pandas as pk\n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('/Users/wei/Documents/physics/code/tnpy'))\n",
    "sys.path.append(os.path.realpath('/Users/wei/Documents/physics/code/tnpy/tnpy'))\n",
    "\n",
    "import tnpy as tp\n",
    "\n",
    "import sympy as sp\n",
    "from sympy import pprint, simplify, expand, latex\n",
    "from IPython.display import display\n",
    "from fractions import Fraction\n",
    "# sp.init_printing (use_latex=\"mathjax\", latex_mode=\"equation\")\n",
    "sp.init_printing(use_latex='mathjax', latex_mode='equation', fontsize='20pt')\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits\n",
    "# from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "class ScalarFormatterForceFormatZero(ticker.ScalarFormatter):\n",
    "    def _set_format(self):  # Override function that finds format to use\n",
    "        self.format = '%1.0f'  # Give format here\n",
    "\n",
    "class ScalarFormatterForceFormatOne(ticker.ScalarFormatter):\n",
    "    def _set_format(self):\n",
    "        self.format = '%1.1f'\n",
    "\n",
    "class ScalarFormatterForceFormat(ticker.ScalarFormatter):\n",
    "    def _set_format(self):\n",
    "        self.format = '%1.2f'\n",
    "        \n",
    "class ScalarFormatterForceFormatThree(ticker.ScalarFormatter):\n",
    "    def _set_format(self):\n",
    "        self.format = '%1.3f'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical physics primer\n",
    "\n",
    "# 2D classical Ising model\n",
    "\n",
    "## TN-rep\n",
    "\n",
    "### On the link\n",
    "\n",
    "### On the original lattice\n",
    "\n",
    "## Exact result for the 2D square Ising model\n",
    "\n",
    "For a 2D classical Ising model with horizontal coupling $J$ and vertical coupling $J^{\\prime}$,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\ln\\left(Z\\right)=-\\beta{F}\n",
    "    =-\\frac{\\ln(2)}{2}-\\frac{1}{2\\pi}\\int_{0}^{\\pi}\\ln\\left[\\cosh(2K)\\cosh(2L)+\\frac{1}{k}\\sqrt{1+k^{2}-2k\\cos(2\\theta)}\\right]d\\theta\n",
    "\\end{equation}\n",
    "$$\n",
    "with $k=1/\\left[\\sinh(2K)\\sinh(2K)\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Free energy F:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{equation}- \\frac{\\int\\limits_{0}^{\\pi} \\log{\\left(\\sqrt{- \\frac{2 \\cos{\\left(2 \\theta \\right)}}{\\sinh^{2}{\\left(2 J \\beta \\right)}} + 1 + \\frac{1}{\\sinh^{4}{\\left(2 J \\beta \\right)}}} \\sinh^{2}{\\left(2 J \\beta \\right)} + \\cosh^{2}{\\left(2 J \\beta \\right)} \\right)}\\, d\\theta}{2 \\pi} - \\frac{\\log{\\left(2 \\right)}}{2}\\end{equation}"
      ],
      "text/plain": [
       "  π                                                                           \n",
       "  ⌠                                                                           \n",
       "  ⎮    ⎛     ___________________________________________                      \n",
       "  ⎮    ⎜    ╱   2⋅cos(2⋅\\theta)               1              2                \n",
       "  ⎮ log⎜   ╱  - ──────────────── + 1 + ──────────────── ⋅sinh (2⋅J⋅\\beta) + co\n",
       "  ⎮    ⎜  ╱         2                      4                                  \n",
       "  ⎮    ⎝╲╱      sinh (2⋅J⋅\\beta)       sinh (2⋅J⋅\\beta)                       \n",
       "  ⌡                                                                           \n",
       "  0                                                                           \n",
       "- ────────────────────────────────────────────────────────────────────────────\n",
       "                                                   2⋅π                        \n",
       "\n",
       "                                  \n",
       "                                  \n",
       "              ⎞                   \n",
       "  2           ⎟                   \n",
       "sh (2⋅J⋅\\beta)⎟ d(\\theta)         \n",
       "              ⎟                   \n",
       "              ⎠                   \n",
       "                                  \n",
       "                            log(2)\n",
       "───────────────────────── - ──────\n",
       "                              2   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{equation}-0.790559070951263\\end{equation}"
      ],
      "text/plain": [
       "-0.790559070951263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Internal energy E:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{equation}- \\frac{\\int\\limits_{0}^{\\pi} \\frac{4 J \\sqrt{- \\frac{2 \\cos{\\left(2 \\theta \\right)}}{\\sinh^{2}{\\left(2 J \\beta \\right)}} + 1 + \\frac{1}{\\sinh^{4}{\\left(2 J \\beta \\right)}}} \\sinh{\\left(2 J \\beta \\right)} \\cosh{\\left(2 J \\beta \\right)} + 4 J \\sinh{\\left(2 J \\beta \\right)} \\cosh{\\left(2 J \\beta \\right)} + \\frac{\\left(\\frac{4 J \\cos{\\left(2 \\theta \\right)} \\cosh{\\left(2 J \\beta \\right)}}{\\sinh^{3}{\\left(2 J \\beta \\right)}} - \\frac{4 J \\cosh{\\left(2 J \\beta \\right)}}{\\sinh^{5}{\\left(2 J \\beta \\right)}}\\right) \\sinh^{2}{\\left(2 J \\beta \\right)}}{\\sqrt{- \\frac{2 \\cos{\\left(2 \\theta \\right)}}{\\sinh^{2}{\\left(2 J \\beta \\right)}} + 1 + \\frac{1}{\\sinh^{4}{\\left(2 J \\beta \\right)}}}}}{\\sqrt{- \\frac{2 \\cos{\\left(2 \\theta \\right)}}{\\sinh^{2}{\\left(2 J \\beta \\right)}} + 1 + \\frac{1}{\\sinh^{4}{\\left(2 J \\beta \\right)}}} \\sinh^{2}{\\left(2 J \\beta \\right)} + \\cosh^{2}{\\left(2 J \\beta \\right)}}\\, d\\theta}{2 \\pi}\\end{equation}"
      ],
      "text/plain": [
       " π                                                                            \n",
       " ⌠                                                                            \n",
       " ⎮                                                                            \n",
       " ⎮                                                                            \n",
       " ⎮          ___________________________________________                       \n",
       " ⎮         ╱   2⋅cos(2⋅\\theta)               1                                \n",
       " ⎮ 4⋅J⋅   ╱  - ──────────────── + 1 + ──────────────── ⋅sinh(2⋅J⋅\\beta)⋅cosh(2\n",
       " ⎮       ╱         2                      4                                   \n",
       " ⎮     ╲╱      sinh (2⋅J⋅\\beta)       sinh (2⋅J⋅\\beta)                        \n",
       " ⎮                                                                            \n",
       " ⎮                                                                            \n",
       " ⎮                                                                            \n",
       "-⎮ ───────────────────────────────────────────────────────────────────────────\n",
       " ⎮                                                                ____________\n",
       " ⎮                                                               ╱   2⋅cos(2⋅\\\n",
       " ⎮                                                              ╱  - ─────────\n",
       " ⎮                                                             ╱         2    \n",
       " ⎮                                                           ╲╱      sinh (2⋅J\n",
       " ⌡                                                                            \n",
       " 0                                                                            \n",
       "──────────────────────────────────────────────────────────────────────────────\n",
       "                                                                              \n",
       "\n",
       "                                                                              \n",
       "                                                                              \n",
       "                                                  ⎛4⋅J⋅cos(2⋅\\theta)⋅cosh(2⋅J⋅\n",
       "                                                  ⎜───────────────────────────\n",
       "                                                  ⎜             3             \n",
       "                                                  ⎝         sinh (2⋅J⋅\\beta)  \n",
       "⋅J⋅\\beta) + 4⋅J⋅sinh(2⋅J⋅\\beta)⋅cosh(2⋅J⋅\\beta) + ────────────────────────────\n",
       "                                                                    __________\n",
       "                                                                   ╱   2⋅cos(2\n",
       "                                                                  ╱  - ───────\n",
       "                                                                 ╱         2  \n",
       "                                                               ╲╱      sinh (2\n",
       "──────────────────────────────────────────────────────────────────────────────\n",
       "_______________________________                                               \n",
       "theta)               1              2                  2                      \n",
       "─────── + 1 + ──────────────── ⋅sinh (2⋅J⋅\\beta) + cosh (2⋅J⋅\\beta)           \n",
       "                  4                                                           \n",
       "⋅\\beta)       sinh (2⋅J⋅\\beta)                                                \n",
       "                                                                              \n",
       "                                                                              \n",
       "──────────────────────────────────────────────────────────────────────────────\n",
       "                           2⋅π                                                \n",
       "\n",
       "                                                         \n",
       "                                                         \n",
       "\\beta)   4⋅J⋅cosh(2⋅J⋅\\beta)⎞     2                      \n",
       "────── - ───────────────────⎟⋅sinh (2⋅J⋅\\beta)           \n",
       "               5            ⎟                            \n",
       "           sinh (2⋅J⋅\\beta) ⎠                            \n",
       "──────────────────────────────────────────────           \n",
       "_________________________________                        \n",
       "⋅\\theta)               1                                 \n",
       "───────── + 1 + ────────────────                         \n",
       "                    4                                    \n",
       "⋅J⋅\\beta)       sinh (2⋅J⋅\\beta)                         \n",
       "────────────────────────────────────────────── d(\\theta) \n",
       "                                                         \n",
       "                                                         \n",
       "                                                         \n",
       "                                                         \n",
       "                                                         \n",
       "                                                         \n",
       "                                                         \n",
       "─────────────────────────────────────────────────────────\n",
       "                                                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{equation}-0.704499070832445\\end{equation}"
      ],
      "text/plain": [
       "-0.704499070832445"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analytical results for Ising model\n",
    "# https://en.wikipedia.org/wiki/Square_lattice_Ising_model\n",
    "\n",
    "b = sp.Symbol(r'\\beta', real=True)\n",
    "t = sp.Symbol(r'\\theta', real=True)\n",
    "J = sp.Symbol('J', real=True)\n",
    "\n",
    "# sp.integrate: to compute the integral\n",
    "# sp.Integral: just create a symbolic integral expression\n",
    "\n",
    "k = 1/(sp.sinh(2*b*J))**2\n",
    "e = -sp.log(2)/2-(1/(2*sp.pi))*sp.Integral(sp.log(sp.cosh(2*b*J)**2+(1/k)*sp.sqrt(1+k**2-2*k*sp.cos(2*t))), (t, 0, sp.pi))\n",
    "# display(e)\n",
    "# display(e.evalf(subs={b: 0.1, J:1.0}))\n",
    "\n",
    "def logZ(b):\n",
    "    k = 1/(sp.sinh(2*b*J))**2\n",
    "    return -sp.log(2)/2-(1/(2*sp.pi))*sp.Integral(sp.log(sp.cosh(2*b*J)**2+(1/k)*sp.sqrt(1+k**2-2*k*sp.cos(2*t))), (t, 0, sp.pi))\n",
    "\n",
    "def U(b):\n",
    "    k = 1/(sp.sinh(2*b*J))**2\n",
    "    return -J*sp.coth(2*b*J)*(1+(2/sp.pi)*(2*(sp.tanh(2*b*J))**2-1)*sp.Integral(1.0/sp.sqrt(1-4*k/((1+k)**2)*(sp.sin(t))**2), (t, 0, sp.pi/2)))\n",
    "\n",
    "beta = 0.3\n",
    "display('Free energy F:', logZ(b))\n",
    "display(logZ(b).evalf(subs={b:beta, J:1.0}))\n",
    "display('Internal energy E:', sp.diff(logZ(b), b))\n",
    "display(sp.diff(logZ(b), b).evalf(subs={b:beta, J:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta free energy internal energy\n",
      "['0.10', -0.703231242285832, -0.203377391097356]\n",
      "['0.12', -0.707722179056315, -0.245871222486703]\n",
      "['0.14', -0.713072929531680, -0.289391258033900]\n",
      "['0.16', -0.719306016790442, -0.334139998248643]\n",
      "['0.18', -0.726448185218975, -0.380337878113544]\n",
      "['0.20', -0.734530812276326, -0.428228833240348]\n",
      "['0.22', -0.743590449273009, -0.478087754153874]\n",
      "['0.24', -0.753669537298383, -0.530230734794032]\n",
      "['0.26', -0.764817367595539, -0.585029577679236]\n",
      "['0.28', -0.777091393948020, -0.642933021814817]\n",
      "['0.30', -0.790559070951263, -0.704499070832446]\n",
      "['0.32', -0.805300513707733, -0.770446689818235]\n",
      "['0.34', -0.821412514067894, -0.841743750716971]\n",
      "['0.36', -0.839014965034055, -0.919769346517525]\n",
      "['0.38', -0.858262002413707, -1.00664930479848]\n",
      "['0.40', -0.879363820774948, -1.10607920374579]\n",
      "['0.42', -0.902639219096205, -1.22605484028068]\n",
      "['0.44', -0.928728540456889, -1.40222696003143]\n",
      "['0.46', -0.958871412868292, -1.58117731752430]\n",
      "['0.48', -0.991524464466794, -1.67772245149649]\n",
      "['0.50', -1.02579281269492, -1.74556457531255]\n",
      "['0.52', -1.06123300089889, -1.79613744892419]\n",
      "['0.54', -1.09756075623056, -1.83500800398791]\n",
      "['0.56', -1.13457749581659, -1.86546866434878]\n",
      "['0.58', -1.17213789145109, -1.88966638225588]\n",
      "['0.60', -1.21013238828841, -1.90908617768408]\n",
      "['0.62', -1.24847666941109, -1.92479654535938]\n",
      "['0.64', -1.28710483484468, -1.93758830548902]\n",
      "['0.66', -1.32596476028841, -1.94805932104942]\n",
      "['0.68', -1.36501482186089, -1.95666911857991]\n",
      "['0.70', -1.40422151972913, -1.96377561233372]\n",
      "['0.72', -1.44355771584880, -1.96966061945867]\n",
      "['0.74', -1.48300130389431, -1.97454806015000]\n",
      "['0.76', -1.52253419073375, -1.97861722247688]\n",
      "['0.78', -1.56214150696515, -1.98201260491067]\n",
      "['0.80', -1.60181098867843, -1.98485133071683]\n",
      "['0.82', -1.64153248901972, -1.98722880579397]\n",
      "['0.84', -1.68129758934849, -1.98922308447273]\n",
      "['0.86', -1.72109928761012, -1.99089827116257]\n",
      "['0.88', -1.76093174712321, -1.99230719342125]\n",
      "['0.90', -1.80079009301676, -1.99349351833983]\n",
      "['0.92', -1.84067024651775, -1.99449343939495]\n",
      "['0.94', -1.88056878949524, -1.99533702897418]\n",
      "['0.96', -1.92048285332802, -1.99604932863849]\n",
      "['0.98', -1.96041002742542, -1.99665123220339]\n",
      "['1.00', -2.00034828370071, -1.99716020411225]\n",
      "['1.02', -2.04029591404648, -1.99759086611514]\n",
      "['1.04', -2.08025147844704, -1.99795547810002]\n",
      "['1.06', -2.12021376182263, -1.99826433344664]\n",
      "['1.08', -2.16018173806341, -1.99852608505415]\n"
     ]
    }
   ],
   "source": [
    "beta, step = 0.1, 0.02\n",
    "num_sample = 50\n",
    "betas = np.linspace(beta, beta+step*num_sample, num_sample, endpoint=False)\n",
    "\n",
    "df = pd.DataFrame(columns=['beta', 'free_ene', 'internal_ene'])\n",
    "print('beta', 'free energy', 'internal energy')\n",
    "for beta in betas:\n",
    "    data = ['{:.2f}'.format(beta), logZ(b).evalf(subs={b:beta, J:1.0}), sp.diff(logZ(b), b).evalf(subs={b:beta, J:1.0})]\n",
    "    print(data)\n",
    "    df.loc[len(df)] = data\n",
    "\n",
    "# df.to_csv('square_classical_ising_exact.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareClassicalIsing(object):\n",
    "    r'''\n",
    "    square lattice of 2D Ising model\n",
    "    '''\n",
    "\n",
    "    def __init__(self, beta: float, J: float, dtype=torch.float64, ctms=None):\n",
    "        r'''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        '''\n",
    "\n",
    "        self._phys_dim = 2\n",
    "        self._beta, self._J = beta, J\n",
    "        \n",
    "        # bond weight matrix\n",
    "        w = torch.tensor([[torch.exp(-beta*J), torch.exp(beta*J)], [torch.exp(beta*J), torch.exp(-beta*J)]])\n",
    "        u, s, v = tp.linalg.svd(w)\n",
    "        m, mp = u @ torch.sqrt(s).diag(), torch.sqrt(s).diag() @ v\n",
    "\n",
    "        # build the site tensor\n",
    "        self._site_tensor = torch.einsum('as,sb,sc,ds->abcd', mp, m, m, mp).to(dtype)\n",
    "\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._ctms = ctms\n",
    "\n",
    "        if self._ctms is not None:\n",
    "            assert ctms[0].dtype == dtype, 'CTM tensor dtype is not matched'\n",
    "            self._rho = self._ctms[0].shape[0]\n",
    "        else:\n",
    "            self._rho = 0\n",
    "\n",
    "    def update_ctms(self, ctms):\n",
    "\n",
    "        self._ctms = ctms\n",
    "        self._rho = self._rho = self._ctms[0].shape[0]\n",
    "\n",
    "        return 1\n",
    "\n",
    "    def ctmrg_mu(self):\n",
    "\n",
    "        mps = [self._ctms[2], self._ctms[5], self._ctms[3]]\n",
    "        mpo = [self._ctms[6], self._site_tensor, self._ctms[7]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ABC,aB->aCA', mpo[0], mps[0])\n",
    "        mpo_mps[1] = torch.einsum('ABCD,efB->eAfCD', mpo[1], mps[1])\n",
    "        mpo_mps[2] = torch.einsum('ABC,aB->aCA', mpo[2], mps[2])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[0], ls[0]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_0 = torch.einsum('abc,cd->abd', ls[0], vt_dagger @ sst_inv)\n",
    "        pl_0 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[0])\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_1 = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl_1 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,abd->dc', mpo_mps[0], pr_0)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl_0, mpo_mps[1], pr_1)\n",
    "        mps[2] = torch.einsum('abc,bcd->ad', pl_1, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[2] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[5] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[3] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_mu_sym(self):\n",
    "        r'''\n",
    "        CTMRG one step move\n",
    "        symmetric projectors according to: https://arxiv.org/abs/1402.2859\n",
    "        '''\n",
    "\n",
    "        mps = [self._ctms[2], self._ctms[5], self._ctms[3]]\n",
    "        mpo = [self._ctms[6], self._site_tensor, self._ctms[7]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ABC,aB->aCA', mpo[0], mps[0])\n",
    "        mpo_mps[1] = torch.einsum('ABCD,efB->eAfCD', mpo[1], mps[1])\n",
    "        mpo_mps[2] = torch.einsum('ABC,aB->aCA', mpo[2], mps[2])\n",
    "\n",
    "        # find symmetric projectors\n",
    "        sym_mpo_mps = [t.clone() for t in mpo_mps]\n",
    "        sym_mpo_mps.insert(1, mpo_mps[1])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = sym_mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[2])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = sym_mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-3], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag().to(self._dtype)\n",
    "        # projectors\n",
    "        pr = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,abd->dc', mpo_mps[0], pr)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "        mps[2] = torch.einsum('abc,bcd->ad', pl, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[2] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[5] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[3] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_mu_test(self):\n",
    "\n",
    "        mps = [self._ctms[2], self._ctms[5], self._ctms[3]]\n",
    "        mpo = [self._ctms[6], self._site_tensor, self._ctms[7]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ABC,aB->aCA', mpo[0], mps[0])\n",
    "        mpo_mps[1] = torch.einsum('ABCD,efB->eAfCD', mpo[1], mps[1])\n",
    "        mpo_mps[2] = torch.einsum('ABC,aB->aCA', mpo[2], mps[2])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[0], ls[0]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag().to(self._dtype)\n",
    "        # projectors\n",
    "        pr_0 = torch.einsum('abc,cd->abd', ls[0], vt_dagger @ sst_inv)\n",
    "        pl_0 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[0])\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag().to(self._dtype)\n",
    "        # projectors\n",
    "        pr_1 = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl_1 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        pr, pl = pr_1, pl_0\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,abd->dc', mpo_mps[0], pr)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "        mps[2] = torch.einsum('abc,bcd->ad', pl, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[2] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[5] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[3] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_md(self):\n",
    "\n",
    "        mps = [self._ctms[0], self._ctms[4], self._ctms[1]]\n",
    "        mpo = [self._ctms[6], self._site_tensor, self._ctms[7]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ab,bcd->adc', mps[0], mpo[0])\n",
    "        mpo_mps[1] = torch.einsum('abc,defc->adbfe', mps[1], mpo[1])\n",
    "        mpo_mps[2] = torch.einsum('ab,bcd->adc', mps[2], mpo[2])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[0], ls[0]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_0 = torch.einsum('abc,cd->abd', ls[0], vt_dagger @ sst_inv)\n",
    "        pl_0 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[0])\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_1 = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl_1 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,abd->dc', mpo_mps[0], pr_0)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl_0, mpo_mps[1], pr_1)\n",
    "        mps[2] = torch.einsum('abc,bcd->ad', pl_1, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[0] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[4] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[1] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_md_sym(self):\n",
    "\n",
    "        mps = [self._ctms[0], self._ctms[4], self._ctms[1]]\n",
    "        mpo = [self._ctms[6], self._site_tensor, self._ctms[7]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ab,bcd->adc', mps[0], mpo[0])\n",
    "        mpo_mps[1] = torch.einsum('abc,defc->adbfe', mps[1], mpo[1])\n",
    "        mpo_mps[2] = torch.einsum('ab,bcd->adc', mps[2], mpo[2])\n",
    "\n",
    "        # find the symmetric projectors\n",
    "        sym_mpo_mps = [t.clone() for t in mpo_mps]\n",
    "        sym_mpo_mps.insert(1, mpo_mps[1])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = sym_mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[2])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = sym_mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-3], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag().to(self._dtype)\n",
    "        # projectors\n",
    "        pr = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,abd->dc', mpo_mps[0], pr)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "        mps[2] = torch.einsum('abc,bcd->ad', pl, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[0] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[4] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[1] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_md_test(self):\n",
    "\n",
    "        mps = [self._ctms[0], self._ctms[4], self._ctms[1]]\n",
    "        mpo = [self._ctms[6], self._site_tensor, self._ctms[7]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ab,bcd->adc', mps[0], mpo[0])\n",
    "        mpo_mps[1] = torch.einsum('abc,defc->adbfe', mps[1], mpo[1])\n",
    "        mpo_mps[2] = torch.einsum('ab,bcd->adc', mps[2], mpo[2])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[0], ls[0]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_0 = torch.einsum('abc,cd->abd', ls[0], vt_dagger @ sst_inv)\n",
    "        pl_0 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[0])\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_1 = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl_1 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        pr, pl = pr_1, pl_0\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,abd->dc', mpo_mps[0], pr)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "        mps[2] = torch.einsum('abc,bcd->ad', pl, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[0] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[4] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[1] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_ml(self):\n",
    "\n",
    "        mps = [self._ctms[0], self._ctms[6], self._ctms[2]]\n",
    "        mpo = [self._ctms[4], self._site_tensor, self._ctms[5]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ab,acd->cbd', mps[0], mpo[0])\n",
    "        mpo_mps[1] = torch.einsum('abc,cdef->afbde', mps[1], mpo[1])\n",
    "        mpo_mps[2] = torch.einsum('ab,acd->cbd', mps[2], mpo[2])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[0], ls[0]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_0 = torch.einsum('abc,cd->abd', ls[0], vt_dagger @ sst_inv)\n",
    "        pl_0 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[0])\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_1 = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl_1 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,bcd->ad', mpo_mps[0], pr_0)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl_0, mpo_mps[1], pr_1)\n",
    "        mps[2] = torch.einsum('abc,dbc->da', pl_1, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[0] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[6] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[2] = mps[2] / torch.linalg.norm(mps[2])\n",
    "        \n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_ml_sym(self):\n",
    "\n",
    "        mps = [self._ctms[0], self._ctms[6], self._ctms[2]]\n",
    "        mpo = [self._ctms[4], self._site_tensor, self._ctms[5]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ab,acd->cbd', mps[0], mpo[0])\n",
    "        mpo_mps[1] = torch.einsum('abc,cdef->afbde', mps[1], mpo[1])\n",
    "        mpo_mps[2] = torch.einsum('ab,acd->cbd', mps[2], mpo[2])\n",
    "\n",
    "        # find the symmetric projectors\n",
    "        sym_mpo_mps = [t.clone() for t in mpo_mps]\n",
    "        sym_mpo_mps.insert(1, mpo_mps[1])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = sym_mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[2])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = sym_mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-3], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag().to(self._dtype)\n",
    "        # projectors\n",
    "        pr = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,bcd->ad', mpo_mps[0], pr)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "        mps[2] = torch.einsum('abc,dbc->da', pl, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[0] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[6] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[2] = mps[2] / torch.linalg.norm(mps[2])\n",
    "        \n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_ml_test(self):\n",
    "\n",
    "        mps = [self._ctms[0], self._ctms[6], self._ctms[2]]\n",
    "        mpo = [self._ctms[4], self._site_tensor, self._ctms[5]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('ab,acd->cbd', mps[0], mpo[0])\n",
    "        mpo_mps[1] = torch.einsum('abc,cdef->afbde', mps[1], mpo[1])\n",
    "        mpo_mps[2] = torch.einsum('ab,acd->cbd', mps[2], mpo[2])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[0], ls[0]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_0 = torch.einsum('abc,cd->abd', ls[0], vt_dagger @ sst_inv)\n",
    "        pl_0 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[0])\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_1 = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl_1 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        pr, pl = pr_1, pl_0\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,bcd->ad', mpo_mps[0], pr)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "        mps[2] = torch.einsum('abc,dbc->da', pl, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[0] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[6] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[2] = mps[2] / torch.linalg.norm(mps[2])\n",
    "        \n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_mr(self):\n",
    "\n",
    "        mps = [self._ctms[1], self._ctms[7], self._ctms[3]]\n",
    "        mpo = [self._ctms[4], self._site_tensor, self._ctms[5]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('abc,bd->adc', mpo[0], mps[0])\n",
    "        mpo_mps[1] = torch.einsum('abcd,efc->edfba', mpo[1], mps[1])\n",
    "        mpo_mps[2] = torch.einsum('abc,bd->adc', mpo[2], mps[2])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[0], ls[0]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_0 = torch.einsum('abc,cd->abd', ls[0], vt_dagger @ sst_inv)\n",
    "        pl_0 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[0])\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag().to(self._dtype)\n",
    "        # projectors\n",
    "        pr_1 = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl_1 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,bcd->ad', mpo_mps[0], pr_0)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl_0, mpo_mps[1], pr_1)\n",
    "        mps[2] = torch.einsum('abc,dbc->da', pl_1, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[1] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[7] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[3] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_mr_sym(self):\n",
    "\n",
    "        mps = [self._ctms[1], self._ctms[7], self._ctms[3]]\n",
    "        mpo = [self._ctms[4], self._site_tensor, self._ctms[5]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('abc,bd->adc', mpo[0], mps[0])\n",
    "        mpo_mps[1] = torch.einsum('abcd,efc->edfba', mpo[1], mps[1])\n",
    "        mpo_mps[2] = torch.einsum('abc,bd->adc', mpo[2], mps[2])\n",
    "\n",
    "        # find the symmetric projectors\n",
    "        sym_mpo_mps = deepcopy(mpo_mps)\n",
    "        sym_mpo_mps.insert(1, mpo_mps[1])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = sym_mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[2])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = sym_mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-3], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag().to(self._dtype)\n",
    "        # projectors\n",
    "        pr = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,bcd->ad', mpo_mps[0], pr)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "        mps[2] = torch.einsum('abc,dbc->da', pl, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[1] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[7] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[3] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctmrg_mr_test(self):\n",
    "\n",
    "        mps = [self._ctms[1], self._ctms[7], self._ctms[3]]\n",
    "        mpo = [self._ctms[4], self._site_tensor, self._ctms[5]]\n",
    "\n",
    "        mpo_mps = [None]*3\n",
    "        mpo_mps[0] = torch.einsum('abc,bd->adc', mpo[0], mps[0])\n",
    "        mpo_mps[1] = torch.einsum('abcd,efc->edfba', mpo[1], mps[1])\n",
    "        mpo_mps[2] = torch.einsum('abc,bd->adc', mpo[2], mps[2])\n",
    "\n",
    "        rs, ls = [], []\n",
    "        temp = mpo_mps[0]\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "        temp = torch.einsum('abc,bcdef->adef', r, mpo_mps[1])\n",
    "        q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "        rs.append(r)\n",
    "\n",
    "        temp = mpo_mps[-1]\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((0,), (1, 2)), qr_dims=(1, 2))\n",
    "        ls.append(l)\n",
    "        temp = torch.einsum('abcde,cdf->abfe', mpo_mps[-2], l)\n",
    "        q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "        ls.append(l)\n",
    "\n",
    "        ls.reverse()\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[0], ls[0]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_0 = torch.einsum('abc,cd->abd', ls[0], vt_dagger @ sst_inv)\n",
    "        pl_0 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[0])\n",
    "\n",
    "        u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "        ut, st, vt = u[:, :self._rho], s[:self._rho], v[:self._rho, :]\n",
    "        ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "        # inverse of square root\n",
    "        sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "        # projectors\n",
    "        pr_1 = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "        pl_1 = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "        pr, pl = pr_1, pl_0\n",
    "\n",
    "        mps = [None]*3\n",
    "        mps[0] = torch.einsum('abc,bcd->ad', mpo_mps[0], pr)\n",
    "        mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "        mps[2] = torch.einsum('abc,dbc->da', pl, mpo_mps[2])\n",
    "\n",
    "        # update CTM tensors\n",
    "        self._ctms[1] = mps[0] / torch.linalg.norm(mps[0])\n",
    "        self._ctms[7] = mps[1] / torch.linalg.norm(mps[1])\n",
    "        self._ctms[3] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def ctm_mag(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def parf(self):\n",
    "\n",
    "        env_l = torch.einsum('ab,bcd,ec->ade', self._ctms[0], self._ctms[6], self._ctms[2])\n",
    "        env_r = torch.einsum('ab,bcd,ec->ade', self._ctms[1], self._ctms[7], self._ctms[3])\n",
    "\n",
    "        temp = env_l.clone()\n",
    "        temp = torch.einsum('abc,ade,bfge,chf->dgh', temp, self._ctms[4], self._site_tensor, self._ctms[5])\n",
    "\n",
    "        return torch.einsum('abc,abc', temp, env_r)\n",
    "\n",
    "    def ctm_bond_energy(self):\n",
    "\n",
    "        # pure bond weight matrix\n",
    "        w = [\n",
    "            [torch.exp(-self._beta*self._J), torch.exp(self._beta*self._J)],\n",
    "            [torch.exp(self._beta*self._J), torch.exp(-self._beta*self._J)],\n",
    "            ]\n",
    "        u, s, v = tp.linalg.svd(torch.tensor(w))\n",
    "        pure_m, pure_mp = u @ torch.sqrt(s).diag(), torch.sqrt(s).diag() @v\n",
    "\n",
    "        # impure bond weight matrix\n",
    "        w = [\n",
    "            [self._J*torch.exp(-self._beta*self._J), -self._J*torch.exp(self._beta*self._J)],\n",
    "            [-self._J*torch.exp(self._beta*self._J), self._J*torch.exp(-self._beta*self._J)],\n",
    "            ]\n",
    "        u, s, v = tp.linalg.svd(torch.tensor(w))\n",
    "        impure_m, impure_mp = u @ torch.sqrt(s).diag(), torch.sqrt(s).diag() @ v\n",
    "\n",
    "        pure_tens = [self._site_tensor]*2\n",
    "\n",
    "        impure_tens = [None]*2\n",
    "        impure_tens[0] = torch.einsum('as,sb,sc,ds->abcd', pure_mp, pure_m, impure_m, pure_mp).to(self._dtype)\n",
    "        impure_tens[1] = torch.einsum('as,sb,sc,ds->abcd', impure_mp, pure_m, pure_m, pure_mp).to(self._dtype)\n",
    "\n",
    "        env_l = torch.einsum('ab,bcd,ec->ade', self._ctms[0], self._ctms[6], self._ctms[2])\n",
    "        env_r = torch.einsum('ab,bcd,ec->ade', self._ctms[1], self._ctms[7], self._ctms[3])\n",
    "\n",
    "        # den\n",
    "        temp = env_l.clone()\n",
    "        temp = torch.einsum('abc,ade,bfge,chf->dgh', temp, self._ctms[4], pure_tens[0], self._ctms[5])\n",
    "        temp = torch.einsum('abc,ade,bfge,chf->dgh', temp, self._ctms[4], pure_tens[1], self._ctms[5])\n",
    "        den = torch.einsum('abc,abc', temp, env_r)\n",
    "\n",
    "        # num\n",
    "        temp = env_l.clone()\n",
    "        temp = torch.einsum('abc,ade,bfge,chf->dgh', temp, self._ctms[4], impure_tens[0], self._ctms[5])\n",
    "        temp = torch.einsum('abc,ade,bfge,chf->dgh', temp, self._ctms[4], impure_tens[1], self._ctms[5])\n",
    "        num = torch.einsum('abc,abc', temp, env_r)\n",
    "\n",
    "        return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor(-0.30969)\n"
     ]
    }
   ],
   "source": [
    "rho = 4\n",
    "\n",
    "# cs = [torch.rand(rho, rho)+1.j*torch.rand(rho, rho) for i in range(4)]\n",
    "# es = [torch.rand(rho, rho, 2)+1.j*torch.rand(rho, rho, 2) for i in range(4)]\n",
    "cs = [torch.rand(rho, rho) for i in range(4)]\n",
    "es = [torch.rand(rho, rho, 2) for i in range(4)]\n",
    "ctms = cs+es\n",
    "\n",
    "beta, step = 0.2, 0.1\n",
    "num_sample = 1\n",
    "betas = np.linspace(beta, beta+step*num_sample, num_sample, endpoint=False)\n",
    "\n",
    "ising = SquareClassicalIsing(beta=torch.tensor(beta), J=1.0, dtype=torch.float64)\n",
    "ising.update_ctms(ctms)\n",
    "\n",
    "print(ising._rho)\n",
    "print(ising.ctm_bond_energy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.41824220657874883\n",
      "1 -0.4274622504598082\n",
      "2 -0.4281839447270691\n",
      "3 -0.4282261288374566\n",
      "4 -0.42822866339756954\n",
      "5 -0.4282288220041636\n",
      "6 -0.428228832465865\n",
      "7 -0.42822883318369204\n",
      "8 -0.42822883323453087\n",
      "9 -0.4282288332382192\n",
      "10 -0.4282288332384912\n",
      "11 -0.4282288332385123\n",
      "12 -0.4282288332385143\n",
      "13 -0.42822883323851446\n",
      "14 -0.42822883323851446\n",
      "15 -0.42822883323851446\n",
      "16 -0.42822883323851446\n",
      "17 -0.42822883323851446\n",
      "18 -0.4282288332385146\n",
      "19 -0.42822883323851446\n",
      "20 -0.42822883323851463\n",
      "21 -0.4282288332385147\n",
      "22 -0.4282288332385146\n",
      "23 -0.4282288332385146\n",
      "24 -0.42822883323851446\n",
      "25 -0.42822883323851463\n",
      "26 -0.4282288332385147\n",
      "27 -0.42822883323851446\n",
      "28 -0.42822883323851463\n",
      "29 -0.4282288332385145\n",
      "30 -0.42822883323851463\n",
      "31 -0.42822883323851463\n"
     ]
    }
   ],
   "source": [
    "num = 32\n",
    "\n",
    "for l in range(num):\n",
    "\n",
    "    ising.ctmrg_mu_sym()\n",
    "    ising.ctmrg_md_sym()\n",
    "    ising.ctmrg_ml_sym()\n",
    "    ising.ctmrg_mr_sym()\n",
    "\n",
    "    bond_ene = ising.ctm_bond_energy()\n",
    "    print(l, bond_ene.item()*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) tensor([0.50000], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{equation}1\\end{equation}"
      ],
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 0.5\n",
    "beta = torch.tensor([beta]).requires_grad_(True)\n",
    "print(beta.shape, beta)\n",
    "\n",
    "ising = SquareClassicalIsing(beta=beta, J=1.0)\n",
    "\n",
    "rho = 8\n",
    "\n",
    "cs = [torch.rand(rho, rho).requires_grad_(True) for i in range(4)]\n",
    "es = [torch.rand(rho, rho, 2).requires_grad_(True) for i in range(4)]\n",
    "ctms = cs+es\n",
    "\n",
    "ising.update_ctms(ctms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 117\u001b[0m\n\u001b[1;32m    113\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(ising\u001b[38;5;241m.\u001b[39mparf())\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f, ising\u001b[38;5;241m.\u001b[39m_ctms\n\u001b[0;32m--> 117\u001b[0m f, ctms \u001b[38;5;241m=\u001b[39m \u001b[43mfree_energy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(f, beta)\n",
      "Cell \u001b[0;32mIn[98], line 104\u001b[0m, in \u001b[0;36mfree_energy\u001b[0;34m(beta, ctms)\u001b[0m\n\u001b[1;32m    100\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num):\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mising\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctmrg_mu_sym\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     ising\u001b[38;5;241m.\u001b[39mctmrg_md_sym()\n\u001b[1;32m    106\u001b[0m     ising\u001b[38;5;241m.\u001b[39mctmrg_ml_sym()\n",
      "Cell \u001b[0;32mIn[46], line 110\u001b[0m, in \u001b[0;36mSquareClassicalIsing.ctmrg_mu_sym\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m mpo_mps[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABC,aB->aCA\u001b[39m\u001b[38;5;124m'\u001b[39m, mpo[\u001b[38;5;241m2\u001b[39m], mps[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# find symmetric projectors\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m sym_mpo_mps \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmpo_mps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m sym_mpo_mps\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m, mpo_mps[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    113\u001b[0m rs, ls \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/copy.py:206\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    204\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m--> 206\u001b[0m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:86\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__deepcopy__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, memo)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly Tensors created explicitly by the user \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(graph leaves) support the deepcopy protocol at the moment.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you were attempting to deepcopy a module, this may be because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof a torch.nn.utils.weight_norm usage, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msee https://github.com/pytorch/pytorch/pull/103001\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001"
     ]
    }
   ],
   "source": [
    "def ctmrg_mu_sym(site_tensor, ctms):\n",
    "    r'''\n",
    "    CTMRG one step move\n",
    "    symmetric projectors according to: https://arxiv.org/abs/1402.2859\n",
    "    '''\n",
    "\n",
    "    rho = ctms[0].shape[0]\n",
    "\n",
    "    mps = [ctms[2], ctms[5], ctms[3]]\n",
    "    mpo = [ctms[6], site_tensor, ctms[7]]\n",
    "\n",
    "    mpo_mps = [None]*3\n",
    "    mpo_mps[0] = torch.einsum('ABC,aB->aCA', mpo[0], mps[0])\n",
    "    mpo_mps[1] = torch.einsum('ABCD,efB->eAfCD', mpo[1], mps[1])\n",
    "    mpo_mps[2] = torch.einsum('ABC,aB->aCA', mpo[2], mps[2])\n",
    "\n",
    "    # find symmetric projectors\n",
    "    sym_mpo_mps = deepcopy(mpo_mps)\n",
    "    sym_mpo_mps.insert(1, mpo_mps[1])\n",
    "\n",
    "    rs, ls = [], []\n",
    "    temp = sym_mpo_mps[0]\n",
    "    q, r = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 0))\n",
    "    rs.append(r)\n",
    "    temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[1])\n",
    "    q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "    rs.append(r)\n",
    "    temp = torch.einsum('abc,bcdef->adef', r, sym_mpo_mps[2])\n",
    "    q, r = tp.linalg.tqr(temp, group_dims=((0, 3), (1, 2)), qr_dims=(1, 0))\n",
    "    rs.append(r)\n",
    "\n",
    "    temp = sym_mpo_mps[-1]\n",
    "    q, l = tp.linalg.tqr(temp, group_dims=((2,), (0, 1)), qr_dims=(1, 2))\n",
    "    ls.append(l)\n",
    "    temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-2], l)\n",
    "    q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "    ls.append(l)\n",
    "    temp = torch.einsum('abcde,cdf->abfe', sym_mpo_mps[-3], l)\n",
    "    q, l = tp.linalg.tqr(temp, group_dims=((2, 3), (0, 1)), qr_dims=(0, 2))\n",
    "    ls.append(l)\n",
    "\n",
    "    ls.reverse()\n",
    "\n",
    "    u, s, v = tp.linalg.svd(torch.einsum('abc,bcd->ad', rs[1], ls[1]))\n",
    "    ut, st, vt = u[:, :rho], s[:rho], v[:rho, :]\n",
    "    ut_dagger, vt_dagger = ut.t().conj(), vt.t().conj()\n",
    "    # inverse of square root\n",
    "    sst_inv = (1.0 / torch.sqrt(st)).diag()\n",
    "    # projectors\n",
    "    pr = torch.einsum('abc,cd->abd', ls[1], vt_dagger @ sst_inv)\n",
    "    pl = torch.einsum('ab,bcd->acd', sst_inv @ ut_dagger, rs[1])\n",
    "\n",
    "    mps = [None]*3\n",
    "    mps[0] = torch.einsum('abc,abd->dc', mpo_mps[0], pr)\n",
    "    mps[1] = torch.einsum('abc,bcdef,deg->agf', pl, mpo_mps[1], pr)\n",
    "    mps[2] = torch.einsum('abc,bcd->ad', pl, mpo_mps[2])\n",
    "\n",
    "    # update CTM tensors\n",
    "    ctms[2] = mps[0] / torch.linalg.norm(mps[0])\n",
    "    ctms[5] = mps[1] / torch.linalg.norm(mps[1])\n",
    "    ctms[3] = mps[2] / torch.linalg.norm(mps[2])\n",
    "\n",
    "    return ctms\n",
    "\n",
    "def ising_free_energy(beta, ctms):\n",
    "\n",
    "    # bond weight matrix\n",
    "    w = torch.tensor([[torch.exp(-beta*J), torch.exp(beta*J)], [torch.exp(beta*J), torch.exp(-beta*J)]])\n",
    "    u, s, v = tp.linalg.svd(w)\n",
    "    m, mp = u @ torch.sqrt(s).diag(), torch.sqrt(s).diag() @ v\n",
    "\n",
    "    # build the site tensor\n",
    "    site_tensor = torch.einsum('as,sb,sc,ds->abcd', mp, m, m, mp)\n",
    "\n",
    "    ising = SquareClassicalIsing(beta=beta, J=1.0)\n",
    "    ising.update_ctms(ctms)\n",
    "\n",
    "    num = 8\n",
    "\n",
    "    for l in range(num):\n",
    "\n",
    "        ising.ctmrg_mu_sym()\n",
    "        ising.ctmrg_md_sym()\n",
    "        ising.ctmrg_ml_sym()\n",
    "        ising.ctmrg_mr_sym()\n",
    "\n",
    "        # bond_ene = ising.ctm_bond_energy()\n",
    "        # print(l, bond_ene.item()*2)\n",
    "\n",
    "    # after num times of RG\n",
    "    f = -1.0*torch.log(ising.parf())\n",
    "\n",
    "    return f, ising._ctms\n",
    "\n",
    "def free_energy(beta, ctms):\n",
    "\n",
    "    ising = SquareClassicalIsing(beta=beta, J=1.0)\n",
    "    ising.update_ctms(ctms)\n",
    "\n",
    "    num = 8\n",
    "\n",
    "    for l in range(num):\n",
    "\n",
    "        ising.ctmrg_mu_sym()\n",
    "        ising.ctmrg_md_sym()\n",
    "        ising.ctmrg_ml_sym()\n",
    "        ising.ctmrg_mr_sym()\n",
    "\n",
    "        # bond_ene = ising.ctm_bond_energy()\n",
    "        # print(l, bond_ene.item()*2)\n",
    "\n",
    "    # after num times of RG\n",
    "    f = -1.0*torch.log(ising.parf())\n",
    "\n",
    "    return f, ising._ctms\n",
    "\n",
    "f, ctms = free_energy(beta, ctms)\n",
    "print(f, beta)\n",
    "# f.backward(inputs=beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
